{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheOnesThatWereAbroad/Assignment1/blob/main/POS_with_workingF1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc8x9pA00Sfd"
      },
      "source": [
        "# Assignment 1 - POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvR73RuF7O7s",
        "outputId": "072b3576-11cd-49fb-e212-fcdf4147bc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHhrkDf2kqzg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import keras_tuner as kt\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
        "\n",
        "from data_input import DataInput\n",
        "from text_vectorizer import TextVectorizer, TargetVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx2qo48Ikqzi"
      },
      "source": [
        "## 1. Build a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN34sX55kqzi"
      },
      "source": [
        "### 1.1 Dataset preparation\n",
        "For this experiment, the [Dependency Parsed Treebank](https://www.nltk.org/nltk_data/) dataset is used.\n",
        "Notice that each document is slitted into sentences, so the dimensionality of the data is more than 199 (total number of documents in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRQ_hwv9mHtK",
        "outputId": "4f9383d0-ccf0-43e2-e206-12c0e7a79586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the dataset...\n",
            "Successful download!\n",
            "Extracting the dataset...\n",
            "Successfully extracted the dataset!\n",
            "Train set size: 1957\n",
            "Dev set size: 979\n",
            "Test set size: 978\n"
          ]
        }
      ],
      "source": [
        "# download the dataset and split it into train, dev and test sets\n",
        "dataset = DataInput(\n",
        "        data_url=\"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\",\n",
        "        train_size=0.50,\n",
        "        dev_size=0.25,\n",
        "        dataset_folder=os.path.join(os.getcwd(), \"dataset\"),\n",
        "        split_into_sentences=True,\n",
        "        shuffle=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGQatFzWkqzk"
      },
      "source": [
        "Pre-processing is always an important step with which start. There are a lot of pre-processing steps that we can consider, but for this experiment the only pre-processing operation performed is:\n",
        "- **to lower**, in part-of-speech tagging scenario casing of the input tokens is crucial to find the correspondent token in the embedding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbVR94oskqzk"
      },
      "outputs": [],
      "source": [
        "# do preprocessing for train, validation and test sets\n",
        "dataset.preprocessing(\"train\", to_lower=True)\n",
        "dataset.preprocessing(\"dev\", to_lower=True)\n",
        "dataset.preprocessing(\"test\", to_lower=True)\n",
        "\n",
        "# separate inputs and targets\n",
        "X_train, y_train = dataset.train\n",
        "X_dev, y_dev = dataset.dev\n",
        "X_test, y_test = dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxfgDx0jrnqV",
        "outputId": "e05c2488-baa5-405f-89df-a119da9a35be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1957,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset.train[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPv-M1xEmHtM"
      },
      "source": [
        "### 1.2 Dataset analysis\n",
        "Let's take a look at the dataset, to inspect the distribution of the POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh9Jg3ePkqzm"
      },
      "outputs": [],
      "source": [
        "def get_occurences(data, ordered=False, exclude=[]):\n",
        "    \"\"\"\n",
        "    Get the occurences of each token in the data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : list of np arrays, representing the data to get the occurences from\n",
        "    ordered : bool, if True, the occurences will be returned in descending order\n",
        "    exclude : list of strings, tokens that should not be included in the occurences\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    occurences : dict, mapping each token to the number of occurences\n",
        "    \"\"\"\n",
        "    tokens, counts = np.unique(np.concatenate(data), return_counts=True)\n",
        "    occurences = dict(zip(tokens, counts))\n",
        "\n",
        "    if len(exclude) > 0:\n",
        "        for token in exclude:\n",
        "            if token in occurences:\n",
        "                del occurences[token]\n",
        "\n",
        "    if ordered:\n",
        "        occurences = dict(sorted(occurences.items(), key=lambda x: x[1], reverse=True))\n",
        "    return occurences\n",
        "\n",
        "# get the occurences of each POS tag in the three sets \n",
        "train_tags_occ = get_occurences(y_train, ordered=True, exclude=[])\n",
        "dev_tags_occ = get_occurences(y_dev, exclude=[])\n",
        "test_tags_occ = get_occurences(y_test, exclude=[])\n",
        "\n",
        "# build a matrix of the occurences of size (n_tags, 3)\n",
        "tags_occ = np.array([[v, dev_tags_occ[k], test_tags_occ[k]] if k in dev_tags_occ and k in test_tags_occ else [v, dev_tags_occ[k], 0] if k in dev_tags_occ else [v, 0, test_tags_occ[k]] if k in test_tags_occ else [v, 0, 0] for k, v in train_tags_occ.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "aZh5QAJIkqzn",
        "outputId": "e52bd3da-ddec-4f20-9350-18d6376fa2f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1656x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot bar chart of the occurences of each POS tag\n",
        "fig, ax = plt.subplots(1, 1, figsize=(23, 8))\n",
        "tags = list(train_tags_occ.keys())\n",
        "ax.bar(tags, tags_occ[:, 1], width=0.3, align='edge', label='dev')\n",
        "ax.bar(tags, tags_occ[:, 2], width=-0.6, align='edge', label='test')\n",
        "ax.bar(tags, tags_occ[:, 0], width=-0.3, align='edge', label='train')\n",
        "ax.set_xticks(tags)\n",
        "ax.set_xlabel('Tags')\n",
        "ax.set_ylabel('Occurences')\n",
        "ax.set_title(\"Occurences of POS tags\")\n",
        "ax.legend()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYCqD_Zpkqzo",
        "outputId": "2a55eec6-cf3b-43e5-c366-1b5aa65f9b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following tags (from the train set) are not in the test set: {'#'}\n",
            "The following tags (from the dev set) are not in the train set: {'SYM'}\n"
          ]
        }
      ],
      "source": [
        "# analyze the difference between the presence of the POS tags in the train, dev and test sets\n",
        "train_tags_not_dev = set(train_tags_occ.keys()) - set(dev_tags_occ.keys())\n",
        "train_tags_not_test = set(train_tags_occ.keys()) - set(test_tags_occ.keys())\n",
        "dev_tags_not_train = set(dev_tags_occ.keys()) - set(train_tags_occ.keys())\n",
        "test_tags_not_train = set(test_tags_occ.keys()) - set(train_tags_occ.keys())\n",
        "\n",
        "if(len(train_tags_not_dev) > 0):\n",
        "    print(\"The following tags (from the train set) are not in the dev set:\", train_tags_not_dev)\n",
        "if(len(train_tags_not_test) > 0):\n",
        "    print(\"The following tags (from the train set) are not in the test set:\", train_tags_not_test)\n",
        "if(len(dev_tags_not_train) > 0):\n",
        "    print(\"The following tags (from the dev set) are not in the train set:\", dev_tags_not_train)\n",
        "if(len(test_tags_not_train) > 0):\n",
        "    print(\"The following tags (from the test set) are not in the train set:\", test_tags_not_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkIYSNxukqzo"
      },
      "source": [
        "### 1.3 GloVe embeddings vectorization\n",
        "The next step consists in downloading a pre-trained embedding model, namely GloVe. It is a techniques that tries to encoded global semantic properties based on the co-occurrence matrix.\n",
        "Three different dimensional space versions are available: 50, 100, 200.\n",
        "\n",
        "From the choosen embedding, a starting vocabulary is built.\n",
        "Then this vocabulary is enriched with the embeddings computed for the OOV words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxU2LR1Bkqzp",
        "outputId": "36824a8a-cbdd-4b55-e8d5-cbbf5d3e753c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the GloVe embeddings...\n",
            "Successful download!\n",
            "Extracting the embeddings...\n",
            "Successfully extracted the embeddings!\n",
            "Parsing the training set...\n",
            "Generated embeddings for 351 OOV words.\n",
            "\n",
            "Parsing the validation set...\n",
            "Generated embeddings for 148 OOV words.\n",
            "\n",
            "Parsing the test set...\n",
            "Generated embeddings for 177 OOV words.\n"
          ]
        }
      ],
      "source": [
        "# initialize the vectorizer for the input tokens to convert them to embedding vectors\n",
        "# and build the vocabulary V1 from the glove embeddings\n",
        "text_vectorizer = TextVectorizer(\n",
        "    glove_url=\"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
        "    max_tokens=20000,\n",
        "    embedding_dim=200,\n",
        "    embedding_folder=os.path.join(os.getcwd(), \"glove\"),\n",
        "    max_size=dataset.max_size\n",
        ")\n",
        "\n",
        "# compute embeddings for terms (OOV1) in the training set that are out of vocabulary V1 and add them: V2=V1+OOV1\n",
        "print(\"Parsing the training set...\")\n",
        "text_vectorizer.adapt(X_train)\n",
        "# use the vocabulary V2 to convert the training set inputs into embedding vectors\n",
        "X_train = text_vectorizer.transform(X_train)\n",
        "\n",
        "# compute embeddings for terms (OOV2) in the validation set that are out of vocabulary V2 and add them: V3=V2+OOV2\n",
        "print(\"\\nParsing the validation set...\")\n",
        "text_vectorizer.adapt(X_dev)\n",
        "# use the vocabulary V3 to convert the validation set inputs into embedding vectors\n",
        "X_dev = text_vectorizer.transform(X_dev)\n",
        "\n",
        "# compute embeddings for terms (OOV3) in the test set that are out of vocabulary V3 and add them: V4=V3+OOV3\n",
        "print(\"\\nParsing the test set...\")\n",
        "text_vectorizer.adapt(X_test)\n",
        "# use the vocabulary V4 to convert the test set inputs into embedding vectors\n",
        "X_test = text_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwDnKWdckqzp",
        "outputId": "050b5f48-b631-412e-9cde-ca22a6d787ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (1957, 249, 200)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Input shape: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiDdPTFrkqzp"
      },
      "source": [
        "### 1.4 One-hot encoding of the targets\n",
        "Each POS tag in each set is converted using the one-hot representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C37qdVk7kqzq"
      },
      "outputs": [],
      "source": [
        "# initialize the vectorizer for the target tags to convert them into one-hot representation\n",
        "target_vectorizer = TargetVectorizer(max_size=dataset.max_size)\n",
        "\n",
        "# adapt the target vectorizer with only the training set: we do not consider possible targets that are not seen in training set but they are in the dev/test set\n",
        "target_vectorizer.adapt(y_train)  \n",
        "\n",
        "# convert the targets into one-hot representation for each splitting set\n",
        "y_train = target_vectorizer.transform(y_train)\n",
        "y_dev = target_vectorizer.transform(y_dev)\n",
        "y_test = target_vectorizer.transform(y_test)\n",
        "\n",
        "n_classes = y_train[0].shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnH7e4okkqzq",
        "outputId": "c4630adf-c8bd-4021-d1dc-5798bb1fad9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target shape: (1957, 249, 44)\n",
            "Number of classes for one-hot encoded targets: 44\n"
          ]
        }
      ],
      "source": [
        "print(f\"Target shape: {y_train.shape}\")\n",
        "print(f\"Number of classes for one-hot encoded targets: {n_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khILeN7fkqzq"
      },
      "source": [
        "## 2. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZfQ0dECkqzr"
      },
      "source": [
        "### 2.1 Models definition\n",
        "We are now ready to define our neural network architectures! \n",
        "Four variants will be evaluated:\n",
        "- Bidirectional LSMT + Dense layer\n",
        "- Bidirectional GRU + Dense layer\n",
        "- Two Bidirectional LSMT + Dense layer\n",
        "- Bidirectional LSMT + Two Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfa_f1 = tfa.metrics.F1Score(\n",
        "    num_classes= n_classes,\n",
        "    name = 'f1_score',\n",
        "    )\n",
        "punctuation_indices = [0, 1, 2, 3, 4, 5, 6,7, 16, 30, 43]\n",
        "\n",
        "def f1_with_tfa(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, (-1, n_classes))\n",
        "    punctuation = tf.gather(y_true, punctuation_indices, axis=-1)\n",
        "    y_mask = tf.cast(tf.logical_not(tf.reduce_any(tf.cast(punctuation, tf.bool), axis=-1)), tf.float32)\n",
        "    y_pred = tf.reshape(y_pred, (-1, n_classes))\n",
        "    tfa_f1.update_state(y_true, y_pred, sample_weight=y_mask)\n",
        "    result = tfa_f1.result()\n",
        "    K.batch_set_value([(v, np.zeros(v.shape.as_list())) for v in tfa_f1.variables])\n",
        "    return np.sum(result)/(n_classes-len(punctuation_indices))"
      ],
      "metadata": {
        "id": "l057_W6_87Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFmjvEd_3ZUK"
      },
      "outputs": [],
      "source": [
        "def build_bilstm_model(hp):\n",
        "    \"\"\"\n",
        "    Build th Bidirectional LSMT + Dense layer model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Masking(name='padding_masking', mask_value=0., input_shape=X_train.shape[1:]))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(hp.Choice('units', [64, 128, 256]), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)), name=\"bilstm_1\"))\n",
        "    model.add(layers.Dropout(hp.Choice('dropout', [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])))\n",
        "    model.add(layers.Dense(n_classes, activation=\"softmax\", name=\"dense_1\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  metrics=[f1_with_tfa])\n",
        "    return model\n",
        "\n",
        "def build_bigru_model(hp):\n",
        "    \"\"\"\n",
        "    Build th Bidirectional GRU + Dense layer model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Masking(name='padding_masking', mask_value=0., input_shape=X_train.shape[1:]))\n",
        "    model.add(layers.Bidirectional(layers.GRU(hp.Choice('units', [64, 128, 256]), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)), name=\"gru_1\"))\n",
        "    model.add(layers.Dropout(hp.Choice('dropout', [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])))\n",
        "    model.add(layers.Dense(n_classes, activation=\"softmax\", name=\"dense_1\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  metrics=[f1_with_tfa])\n",
        "    return model\n",
        "\n",
        "def build_two_bilstm_model(hp):\n",
        "    \"\"\"\n",
        "    Build the Two Bidirectional LSTM + Dense layer model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Masking(name='padding_masking', mask_value=0., input_shape=X_train.shape[1:]))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(hp.Choice('units1', [64, 128, 256]), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)), name=\"bilstm_1\"))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(hp.Choice('units2', [64, 128, 256]), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)), name=\"bilstm_2\"))\n",
        "    model.add(layers.Dropout(hp.Choice('dropout', [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])))\n",
        "    model.add(layers.Dense(n_classes, activation=\"softmax\", name=\"dense_1\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  metrics=[f1_with_tfa])\n",
        "    return model\n",
        "\n",
        "def build_bilstm_two_dense_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Masking(name='padding_masking', mask_value=0., input_shape=X_train.shape[1:]))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(hp.Choice('units1', [64, 128, 256]), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)), name=\"bilstm_1\"))\n",
        "    model.add(layers.Dropout(hp.Choice('dropout1', [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])))\n",
        "    model.add(layers.Dense(hp.Choice('units2', [64, 128, 256]), activation=\"relu\", name=\"dense_1\", kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "    model.add(layers.Dropout(hp.Choice('dropout2', [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])))\n",
        "    model.add(layers.Dense(n_classes, activation=\"softmax\", name=\"dense_2\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  metrics=[f1_with_tfa])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0R3goy83ZUK"
      },
      "source": [
        "### 2.2 Hyperparameter tuning\n",
        "Let's do a tuning of the hyperparameters for each variant of the architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5sOwkYf3ZUK"
      },
      "outputs": [],
      "source": [
        "epochs_tuning = 100\n",
        "batch_size_tuning = 64\n",
        "callbacks_tuning = [keras.callbacks.EarlyStopping(monitor='val_f1_with_tfa', patience=10, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sErnjwr3ZUL"
      },
      "source": [
        "Model 1: Bidirectional LSMT + Dense layer variant tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3vU8lwL3ZUL",
        "outputId": "70b39dea-36b4-4c67-f6ed-904b887f50c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 52 Complete [00h 00m 22s]\n",
            "val_f1_with_tfa: 0.026384983211755753\n",
            "\n",
            "Best val_f1_with_tfa So Far: 0.24316705763339996\n",
            "Total elapsed time: 00h 21m 54s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "The hyperparameter search is complete.\n",
            "The optimal number of units in the LSTM layer is 64.\n",
            "The optimal rate for Dropout layer is 0.3.\n",
            "The optimal learning rate for the optimizer is 0.01.\n"
          ]
        }
      ],
      "source": [
        "# tune the hyperparameters of the first model\n",
        "tuner_bilstm = kt.Hyperband(build_bilstm_model,\n",
        "                     objective=kt.Objective(\"val_f1_with_tfa\", direction=\"max\"),\n",
        "                     max_epochs=epochs_tuning,\n",
        "                     overwrite=True,\n",
        "                     directory=\"tuner\",\n",
        "                     project_name=\"tuner_bilstm\")\n",
        "tuner_bilstm.search(X_train, y_train, epochs=epochs_tuning, batch_size=batch_size_tuning, validation_data=(X_dev, y_dev), callbacks=callbacks_tuning)\n",
        "best_hps_bilstm = tuner_bilstm.get_best_hyperparameters()[0]\n",
        "\n",
        "print(f\"The hyperparameter search is complete.\\n\" \n",
        "      f\"The optimal number of units in the LSTM layer is {best_hps_bilstm.get('units')}.\\n\" \n",
        "      f\"The optimal rate for Dropout layer is {best_hps_bilstm.get('dropout')}.\\n\" \n",
        "      f\"The optimal learning rate for the optimizer is {best_hps_bilstm.get('learning_rate')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPb8yMfz3ZUL"
      },
      "source": [
        "Model 2: Bidirectional GRU + Dense layer variant tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_tLVIkM3ZUL",
        "outputId": "48f180a8-7c0f-4845-c927-8e0cd8505e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project tuner/tuner_bigru/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from tuner/tuner_bigru/tuner0.json\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "The hyperparameter search is complete.\n",
            "The optimal number of units in the GRU layer is 256.\n",
            "The optimal rate for Dropout layer is 0.3.\n",
            "The optimal learning rate for the optimizer is 0.01.\n"
          ]
        }
      ],
      "source": [
        "# tune the hyperparameters of the second model\n",
        "tuner_bigru = kt.Hyperband(build_bigru_model,\n",
        "                     objective=kt.Objective(\"val_f1_with_tfa\", direction=\"max\"),\n",
        "                     max_epochs=epochs_tuning,\n",
        "                     #overwrite=True,\n",
        "                     directory=\"tuner\",\n",
        "                     project_name=\"tuner_bigru\")\n",
        "tuner_bigru.search(X_train, y_train, epochs=epochs_tuning, batch_size=batch_size_tuning, validation_data=(X_dev, y_dev), callbacks=callbacks_tuning)\n",
        "best_hps_bigru = tuner_bigru.get_best_hyperparameters()[0]\n",
        "\n",
        "print(f\"The hyperparameter search is complete.\\n\" \n",
        "      f\"The optimal number of units in the GRU layer is {best_hps_bigru.get('units')}.\\n\" \n",
        "      f\"The optimal rate for Dropout layer is {best_hps_bigru.get('dropout')}.\\n\" \n",
        "      f\"The optimal learning rate for the optimizer is {best_hps_bigru.get('learning_rate')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIp3D6c-3ZUM"
      },
      "source": [
        "Model 3: Two Bidirectional LSMT + Dense layer variant tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXxWaco93ZUM",
        "outputId": "ae82257e-b209-490d-982d-4f3fb5156747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 202 Complete [00h 01m 46s]\n",
            "val_f1_with_tfa: 0.018025290220975876\n",
            "\n",
            "Best val_f1_with_tfa So Far: 0.13028207421302795\n",
            "Total elapsed time: 01h 51m 57s\n",
            "\n",
            "Search: Running Trial #203\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "256               |256               |units1\n",
            "256               |256               |units2\n",
            "0.5               |0.4               |dropout\n",
            "0.01              |0.001             |learning_rate\n",
            "12                |34                |tuner/epochs\n",
            "4                 |12                |tuner/initial_epoch\n",
            "3                 |4                 |tuner/bracket\n",
            "1                 |3                 |tuner/round\n",
            "0166              |0131              |tuner/trial_id\n",
            "\n",
            "Epoch 5/12\n",
            "31/31 [==============================] - 33s 399ms/step - loss: 2.7431 - f1_with_tfa: 0.0130 - val_loss: 0.5173 - val_f1_with_tfa: 0.0110\n",
            "Epoch 6/12\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.3488 - f1_with_tfa: 0.0156 - val_loss: 0.2854 - val_f1_with_tfa: 0.0138\n",
            "Epoch 7/12\n",
            "31/31 [==============================] - 5s 159ms/step - loss: 0.2748 - f1_with_tfa: 0.0157 - val_loss: 0.2758 - val_f1_with_tfa: 0.0137\n",
            "Epoch 8/12\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.2698 - f1_with_tfa: 0.0157 - val_loss: 0.2737 - val_f1_with_tfa: 0.0161\n",
            "Epoch 9/12\n",
            "31/31 [==============================] - 5s 159ms/step - loss: 0.2691 - f1_with_tfa: 0.0162 - val_loss: 0.2724 - val_f1_with_tfa: 0.0155\n",
            "Epoch 10/12\n",
            "31/31 [==============================] - 5s 163ms/step - loss: 0.2670 - f1_with_tfa: 0.0160 - val_loss: 0.2715 - val_f1_with_tfa: 0.0163\n",
            "Epoch 11/12\n",
            "31/31 [==============================] - 5s 160ms/step - loss: 0.2667 - f1_with_tfa: 0.0166 - val_loss: 0.2712 - val_f1_with_tfa: 0.0156\n",
            "Epoch 12/12\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.2663 - f1_with_tfa: 0.0164 - val_loss: 0.2707 - val_f1_with_tfa: 0.0181\n"
          ]
        }
      ],
      "source": [
        "# tune the hyperparameters of the third model\n",
        "tuner_two_bilstm = kt.Hyperband(build_two_bilstm_model,\n",
        "                     objective=kt.Objective(\"val_f1_with_tfa\", direction=\"max\"),\n",
        "                     max_epochs=epochs_tuning,\n",
        "                     #overwrite=True,\n",
        "                     directory=\"tuner\",\n",
        "                     project_name=\"tuner_two_bilstm\")\n",
        "tuner_two_bilstm.search(X_train, y_train, epochs=epochs_tuning, batch_size=batch_size_tuning, validation_data=(X_dev, y_dev), callbacks=callbacks_tuning)\n",
        "best_hps_two_bilstm = tuner_two_bilstm.get_best_hyperparameters()[0]\n",
        "\n",
        "print(f\"The hyperparameter search is complete.\\n\" \n",
        "      f\"The optimal number of units in the first LSTM layer is {best_hps_two_bilstm.get('units1')}.\\n\" \n",
        "      f\"The optimal number of units in the second LSTM layer is {best_hps_two_bilstm.get('units2')}.\\n\" \n",
        "      f\"The optimal rate for Dropout layer is {best_hps_two_bilstm.get('dropout')}.\\n\" \n",
        "      f\"The optimal learning rate for the optimizer is {best_hps_two_bilstm.get('learning_rate')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMNWw_i33ZUM"
      },
      "source": [
        "Model 4: Bidirectional LSMT + two Dense layer variant tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHSGHizM3ZUM"
      },
      "outputs": [],
      "source": [
        "# tune the hyperparameters of the fourth model\n",
        "tuner_bilstm_two_dense = kt.Hyperband(build_bilstm_two_dense_model,\n",
        "                     objective=kt.Objective(\"val_f1_with_tfa\", direction=\"max\"),\n",
        "                     max_epochs=epochs_tuning,\n",
        "                     #overwrite=True,\n",
        "                     directory=\"tuner\",\n",
        "                     project_name=\"tuner_bilstm_two_dense\")\n",
        "tuner_bilstm_two_dense.search(X_train, y_train, epochs=epochs_tuning, batch_size=batch_size_tuning, validation_data=(X_dev, y_dev), callbacks=callbacks_tuning)\n",
        "best_hps_bilstm_two_dense = tuner_bilstm_two_dense.get_best_hyperparameters()[0]\n",
        "\n",
        "print(f\"The hyperparameter search is complete.\\n\" \n",
        "      f\"The optimal number of units in the LSTM layer is {best_hps_bilstm_two_dense.get('units1')}.\\n\" \n",
        "      f\"The optimal rate for the first Dropout layer is {best_hps_bilstm_two_dense.get('dropout1')}.\\n\" \n",
        "      f\"The optimal number of units in first densely-connected layer is {best_hps_bilstm_two_dense.get('units2')}.\\n\" \n",
        "      f\"The optimal rate for the second Dropout layer is {best_hps_bilstm_two_dense.get('dropout2')}.\\n\"\n",
        "      f\"The optimal learning rate for the optimizer is {best_hps_bilstm_two_dense.get('learning_rate')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWdZM3rskqzr"
      },
      "source": [
        "### 2.3 Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGdlAMOu3ZUN"
      },
      "outputs": [],
      "source": [
        "epochs_training = 100\n",
        "batch_size_training = 64\n",
        "callbacks_training = [keras.callbacks.EarlyStopping(monitor='val_f1_with_tfa', patience=10, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CkJaP7_3ZUN"
      },
      "outputs": [],
      "source": [
        "def create_model(layers_info, compile_info):\n",
        "    \"\"\"\n",
        "    Create a Keras model given a list of layer information\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers_info : a list of dictionaries, one for each layer\n",
        "    compile_info : dictionary containing compile information\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: the built keras sequential model\n",
        "    \"\"\"\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    for info in layers_info:\n",
        "        layer = info['layer_name'](**{key: value for key, value in info.items() if key != 'layer_name'})\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNvSu_g23ZUN"
      },
      "source": [
        "Model 1: Bidirectional LSMT + Dense layer variant training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f57Z0qk03ZUO"
      },
      "outputs": [],
      "source": [
        "# first model    \n",
        "layers_info_bilstm = [\n",
        "    {\n",
        "        \"layer_name\": layers.Masking,\n",
        "        \"name\": \"padding_masking\",\n",
        "        \"mask_value\": 0.,\n",
        "        \"input_shape\": X_train.shape[1:]\n",
        "    },\n",
        "    {\n",
        "        'layer_name': layers.Bidirectional,\n",
        "        \"layer\": layers.LSTM(best_hps_bilstm.get('units'), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        \"name\": \"bilstm_1\",\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dropout,\n",
        "        \"rate\": best_hps_bilstm.get('dropout'),\n",
        "        \"name\": \"dropout_1\"\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dense,\n",
        "        \"units\": n_classes,\n",
        "        \"activation\": \"softmax\",\n",
        "        \"name\": \"dense_1\"\n",
        "    }\n",
        "]\n",
        "\n",
        "compile_info_bilstm = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=best_hps_bilstm.get('learning_rate')),\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'metrics' : [f1_with_tfa]\n",
        "}\n",
        "\n",
        "model_bilstm = create_model(layers_info_bilstm, compile_info_bilstm)\n",
        "model_bilstm.summary()\n",
        "\n",
        "# start training\n",
        "history_bilstm = model_bilstm.fit(X_train, y_train, epochs=epochs_training, batch_size=batch_size_training, validation_data=(X_dev, y_dev), callbacks=callbacks_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2QWE-m03ZUO"
      },
      "source": [
        "Model 2: Bidirectional GRU + Dense layer variant training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXZNNrlO3ZUO"
      },
      "outputs": [],
      "source": [
        "# second model    \n",
        "layers_info_bigru = [\n",
        "    {\n",
        "        \"layer_name\": layers.Masking,\n",
        "        \"name\": \"padding_masking\",\n",
        "        \"mask_value\": 0.,\n",
        "        \"input_shape\": X_train.shape[1:]\n",
        "    },\n",
        "    {\n",
        "        'layer_name': layers.Bidirectional,\n",
        "        \"layer\": layers.GRU(best_hps_bigru.get('units'), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        \"name\": \"bigru_1\",\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dropout,\n",
        "        \"rate\": best_hps_bigru.get('dropout'),\n",
        "        \"name\": \"dropout_1\"\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dense,\n",
        "        \"units\": n_classes,\n",
        "        \"activation\": \"softmax\",\n",
        "        \"name\": \"dense_1\"\n",
        "    }\n",
        "]\n",
        "\n",
        "compile_info_bigru = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=best_hps_bigru.get('learning_rate')),\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'metrics' : [f1_with_tfa]\n",
        "}\n",
        "\n",
        "model_bigru = create_model(layers_info_bigru, compile_info_bigru)\n",
        "model_bigru.summary()\n",
        "\n",
        "# start training\n",
        "history_bigru = model_bigru.fit(X_train, y_train, epochs=epochs_training, batch_size=batch_size_training, validation_data=(X_dev, y_dev),  callbacks=callbacks_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P52dWFq63ZUO"
      },
      "source": [
        "Model 3: Two Bidirectional LSMT + Dense layer variant training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzTLBLdC3ZUO"
      },
      "outputs": [],
      "source": [
        "# third model    \n",
        "layers_info_two_bilstm = [\n",
        "    {\n",
        "        \"layer_name\": layers.Masking,\n",
        "        \"name\": \"padding_masking\",\n",
        "        \"mask_value\": 0.,\n",
        "        \"input_shape\": X_train.shape[1:]\n",
        "    },\n",
        "    {\n",
        "        'layer_name': layers.Bidirectional,\n",
        "        \"layer\": layers.LSTM(best_hps_two_bilstm.get('units1'), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        \"name\": \"bilstm_1\",\n",
        "    },\n",
        "    {\n",
        "        'layer_name': layers.Bidirectional,\n",
        "        \"layer\": layers.LSTM(best_hps_two_bilstm.get('units2'), return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        \"name\": \"bilstm_2\",\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dropout,\n",
        "        \"rate\": best_hps_two_bilstm.get('dropout'),\n",
        "        \"name\": \"dropout_1\"\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dense,\n",
        "        \"units\": n_classes,\n",
        "        \"activation\": \"softmax\",\n",
        "        \"name\": \"dense_1\"\n",
        "    }\n",
        "]\n",
        "\n",
        "compile_info_two_bilstm = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=best_hps_two_bilstm.get('learning_rate')),\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'metrics' : [f1_with_tfa]\n",
        "}\n",
        "\n",
        "model_two_bilstm = create_model(layers_info_two_bilstm, compile_info_two_bilstm)\n",
        "model_two_bilstm.summary()\n",
        "\n",
        "# start training\n",
        "history_two_bilstm = model_two_bilstm.fit(X_train, y_train, epochs=epochs_training, batch_size=batch_size_training, validation_data=(X_dev, y_dev), callbacks=callbacks_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDri7iKq3ZUP"
      },
      "source": [
        "Model 4: Bidirectional LSMT + two Dense layer variant training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrxBTqOQ3ZUP"
      },
      "outputs": [],
      "source": [
        "# fourth model    \n",
        "layers_info_bilstm_two_dense = [\n",
        "    {\n",
        "        \"layer_name\": layers.Masking,\n",
        "        \"name\": \"padding_masking\",\n",
        "        \"mask_value\": 0.,\n",
        "        \"input_shape\": X_train.shape[1:]\n",
        "    },\n",
        "    {\n",
        "        'layer_name': layers.Bidirectional,\n",
        "        \"layer\": layers.LSTM(best_hps_bilstm_two_dense.get('units1'), return_sequences=True),\n",
        "        \"name\": \"bilstm_1\",\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dropout,\n",
        "        \"rate\": best_hps_bilstm_two_dense.get('dropout1'),\n",
        "        \"name\": \"dropout_1\"\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dense,\n",
        "        \"units\": best_hps_bilstm_two_dense.get('units2'),\n",
        "        \"activation\": \"relu\",\n",
        "        \"kernel_regularizer\": keras.regularizers.l2(0.01),\n",
        "        \"name\": \"dense_1\"\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dropout,\n",
        "        \"rate\": best_hps_bilstm_two_dense.get('dropout2'),\n",
        "        \"name\": \"dropout_2\"\n",
        "    },\n",
        "    {\n",
        "        \"layer_name\": layers.Dense,\n",
        "        \"units\": n_classes,\n",
        "        \"activation\": \"softmax\",\n",
        "        \"name\": \"dense_2\"\n",
        "    }\n",
        "]\n",
        "\n",
        "compile_info_bilstm_two_dense = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=best_hps_bilstm_two_dense.get('learning_rate')),\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'metrics' : [f1_with_tfa]\n",
        "}\n",
        "\n",
        "model_bilstm_two_dense = create_model(layers_info_bilstm_two_dense, compile_info_bilstm_two_dense)\n",
        "model_bilstm_two_dense.summary()\n",
        "\n",
        "# start training\n",
        "history_bilstm_two_dense = model_bilstm_two_dense.fit(X_train, y_train, epochs=epochs_training, batch_size=batch_size_training, validation_data=(X_dev, y_dev), callbacks=callbacks_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NnTW1ct3ZUP"
      },
      "source": [
        "#### 2.3.1 Store or load all the trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A29NOAda3ZUP"
      },
      "source": [
        "Store weights and history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp38wlcW3ZUP"
      },
      "outputs": [],
      "source": [
        "# store models weights\n",
        "path_model_weights = os.path.join(os.getcwd(), 'models', 'weights')\n",
        "model_bilstm.save_weights(os.path.join(path_model_weights, 'weights_bilstm.h5'))\n",
        "model_bigru.save_weights(os.path.join(path_model_weights, 'weights_bigru.h5'))\n",
        "model_two_bilstm.save_weights(os.path.join(path_model_weights, 'weights_two_bilstm.h5'))\n",
        "model_bilstm_two_dense.save_weights(os.path.join(path_model_weights, 'weights_bilstm_two_dense.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiLuH0Px3ZUQ"
      },
      "outputs": [],
      "source": [
        "# convert the history.history dict to a pandas DataFrame: \n",
        "path_model_weights = os.path.join(os.getcwd(), 'models', 'history')\n",
        "    \n",
        "df_history_bilstm = pd.DataFrame(history_bilstm.history) \n",
        "with open(os.path.join(path_model_weights, \"history_bilstm.csv\"), mode=\"w\") as file:\n",
        "    df_history_bilstm.to_csv(file)\n",
        "\n",
        "df_history_bigru = pd.DataFrame(history_bigru.history)\n",
        "with open(os.path.join(path_model_weights, \"history_bigru.csv\"), mode=\"w\") as file:\n",
        "    df_history_bigru.to_csv(file)\n",
        "\n",
        "df_history_two_bilstm = pd.DataFrame(history_two_bilstm.history)\n",
        "with open(os.path.join(path_model_weights, \"history_two_bilstm.csv\"), mode=\"w\") as file:\n",
        "    df_history_two_bilstm.to_csv(file)\n",
        "\n",
        "df_history_bilstm_two_dense = pd.DataFrame(history_bilstm_two_dense.history)\n",
        "with open(os.path.join(path_model_weights, \"history_bilstm_two_dense.csv\"), mode=\"w\") as file:\n",
        "    df_history_bilstm_two_dense.to_csv(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQy5JgGT3ZUQ"
      },
      "source": [
        "Read weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_puUbUNv3ZUQ"
      },
      "outputs": [],
      "source": [
        "# Restore the weights\n",
        "path_model_weights = os.path.join(os.getcwd(), 'models', 'weights')\n",
        "model_bilstm.load_weights(os.path.join(path_model_weights, 'weights_bilstm.h5'))\n",
        "model_bigru.load_weights(os.path.join(path_model_weights, 'weights_bigru.h5'))\n",
        "model_two_bilstm.load_weights(os.path.join(path_model_weights, 'weights_two_bilstm.h5'))\n",
        "model_bilstm_two_dense.load_weights(os.path.join(path_model_weights, 'weights_bilstm_two_dense.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab9ETsbC2Q82"
      },
      "source": [
        "### 2.4 History of the training\n",
        "Plot metrics evolution for each epoch during the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25YBLK3E2R6m"
      },
      "outputs": [],
      "source": [
        "def plot_history(models_history, keys, model_names=[], labels=(\"epochs\", \"metrics\"), figsize=(10,5), cmap='rainbow'):\n",
        "    \"\"\"\n",
        "    Plot the history of the metrics in the history dictionary for each model.\n",
        "        :param models_history: array of dictionary of the metric history for each model\n",
        "        :param keys: list of keys of the metrics to plot\n",
        "        :param model_names: list of names of the models\n",
        "        :param labels: list of labels of the axes\n",
        "        :param figsize: size of the figure\n",
        "        :param cmap: color map used for the plot\n",
        "    \"\"\"\n",
        "\n",
        "    # maps each model to a distinct RGB color\n",
        "    cmap = plt.cm.get_cmap(cmap, len(models_history))\n",
        "\n",
        "    \n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    # for each model trained\n",
        "    for i, history in enumerate(models_history):\n",
        "        # take all pairs of training and val metrics\n",
        "        for j in range(0, len(keys), 2):\n",
        "            metric, val_metric = keys[j], keys[j+1]\n",
        "            \n",
        "            plt.plot(history[metric], label=f\"{model_names[i]} {metric}\", linestyle=\"solid\", color=cmap(i))\n",
        "            plt.plot(history[val_metric],  label=f\"{model_names[i]} {val_metric}\", linestyle=\"--\",  color=cmap(i))\n",
        "        \n",
        "    plt.xlabel(labels[0])\n",
        "    plt.ylabel(labels[1])\n",
        "\n",
        "    # Adding legend\n",
        "    plt.legend(\n",
        "          title =\"Legend\",\n",
        "          loc =\"upper left\",\n",
        "          bbox_to_anchor =(1.0, 0, 0.5, 1))\n",
        "    plt.title(\"Training history\")\n",
        "    plt.grid(linestyle='--', linewidth=1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca9TMoj42VgT"
      },
      "outputs": [],
      "source": [
        "models_history = [history_bilstm.history, history_bigru.history, history_two_bilstm.history, history_bilstm_two_dense.history]\n",
        "model_names = [\"BiLSTM\", \"BiGRU\", \"2 BiLSTM\", \"BiLSTM + 2 Dense\"]\n",
        "plot_history(models_history, keys=['loss', 'val_loss'], model_names=model_names, labels=(\"epochs\", \"loss\"), figsize=(15,7),  cmap='bwr')\n",
        "plot_history(models_history, keys=['f1_with_tfa', 'val_f1_with_tfa'], model_names=model_names, labels=(\"epochs\", \"f1\"), figsize=(15,7),  cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmIWLatvkqzt"
      },
      "source": [
        "## 3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def adapt_output_for_evaluation(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  given true outputs and predicted ones, this function returns\n",
        "  both of them in 1D and as classes and not probability distributions\n",
        "  \"\"\"\n",
        "  len_sentence = y_pred.shape[1]\n",
        "  num_sentences = y_pred.shape[0]\n",
        "  pred_val = np.empty((num_sentences, len_sentence))\n",
        "  true_val = np.empty((num_sentences, len_sentence))\n",
        "  # assign label with the highest probability\n",
        "  for i in range(num_sentences):\n",
        "      for j in range(len_sentence):\n",
        "          pred_val[i,j] = np.argmax(y_pred[i,j,:])\n",
        "          true_val[i,j] = np.argmax(y_true[i,j,:])\n",
        "  # flatten the numpy array to have a 1D array\n",
        "  true_val = true_val.flatten()\n",
        "  pred_val = pred_val.flatten()\n",
        "  return true_val, pred_val"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Xy9kdn1NhaJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def show_classification_report(model, X_test, y_test, labels, names, sorted_by=None, ascending=True):\n",
        "  \"\"\"\n",
        "  Show classification report using only classes in names\n",
        "  \"\"\"\n",
        "  raw_y_true = np.array(y_test)\n",
        "  raw_y_pred = model.predict(X_test)\n",
        "  y_true, y_pred = adapt_output_for_evaluation(raw_y_true, raw_y_pred)\n",
        "  dictionary =  classification_report(y_true, y_pred, labels=labels, target_names=names, output_dict=True, zero_division=0)\n",
        "  # global performance\n",
        "  keys = ['weighted avg', 'micro avg', 'macro avg']\n",
        "  global_dict = {key: dictionary[key] for key in keys}\n",
        "  for key in keys:\n",
        "    del dictionary[key]\n",
        "\n",
        "  df = pd.DataFrame.from_dict(dictionary).transpose().round(2)\n",
        "  if sorted_by:\n",
        "    df.sort_values(by=sorted_by, ascending=ascending, inplace=True)\n",
        "  print(df)\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  df_glob = pd.DataFrame.from_dict(global_dict).transpose().round(2)\n",
        "  print(df_glob)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4Amf3FrOhaJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q71LkwAPkqzt"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, labels):\n",
        "    \"\"\"\n",
        "    given a trained model and a test set returns the f-score and the confusion matrix\n",
        "    taking into account only classes in labels\n",
        "    \"\"\"\n",
        "    raw_y_true = np.array(y_test)\n",
        "    raw_y_pred = model.predict(X_test)\n",
        "    y_true, y_pred = adapt_output_for_evaluation(raw_y_true, raw_y_pred)\n",
        "    # show confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    disp = ConfusionMatrixDisplay(conf_matrix)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    disp.plot(ax=ax)\n",
        "    # f1 score\n",
        "    print(\"F score:\\n-------------------------------\\n\")\n",
        "    print(sklearn.metrics.f1_score(y_true, y_pred, labels=labels, average='macro', zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua2c1KdCkqzt"
      },
      "outputs": [],
      "source": [
        "punctuation_indexes = [0, 1, 2, 3, 4, 5, 6, 7, 16, 30, 43]\n",
        "classes = target_vectorizer.get_classes()\n",
        "valid_labels = list(set(range(len(classes))) - set(punctuation_indexes))\n",
        "\n",
        "# confusion matrix and global f1 score\n",
        "evaluate_model(model_bilstm, X_test, y_test, valid_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# classification report using only meaningful classes\n",
        "meaningful_classes = classes[valid_labels]\n",
        "show_classification_report(model_bilstm, X_test, y_test, valid_labels, meaningful_classes, sorted_by=['f1-score'], ascending=False)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QUH_GMRUhaJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFaG0yb4kqzu"
      },
      "outputs": [],
      "source": [
        "i_test = 6  # change this to see the prediction for a different sentence\n",
        "\n",
        "print(\"Original POS tagging: \",target_vectorizer.inverse_transform([y_test[i_test]])[0])\n",
        "\n",
        "raw_y_pred = model_bilstm_two_dense.predict(np.array([X_test[i_test]]))\n",
        "# shape of the output is doc x len_sen x classes\n",
        "# argmax for label predictions\n",
        "\n",
        "print(\"Predicted POS tagging: \",target_vectorizer.inverse_transform_probabilities(raw_y_pred)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of POS_tagging.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}