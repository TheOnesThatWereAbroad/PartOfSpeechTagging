{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse.dependencygraph import DependencyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /Users/simone/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('dependency_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(will\n",
      "  (Vinken Pierre , (old (years 61)) ,)\n",
      "  (join (board the) (as (director a nonexecutive)) (Nov. 29))\n",
      "  .)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "t = dependency_treebank.parsed_sents()\n",
    "print(t[0].tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the GloVe embeddings to vectorize our data.\n",
    "\n",
    "**Uncomment to actually download!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0095.dp',\n",
       " 'wsj_0184.dp',\n",
       " 'wsj_0177.dp',\n",
       " 'wsj_0037.dp',\n",
       " 'wsj_0126.dp',\n",
       " 'wsj_0066.dp',\n",
       " 'wsj_0052.dp',\n",
       " 'wsj_0112.dp',\n",
       " 'wsj_0003.dp',\n",
       " 'wsj_0143.dp',\n",
       " 'wsj_0153.dp',\n",
       " 'wsj_0013.dp',\n",
       " 'wsj_0102.dp',\n",
       " 'wsj_0042.dp',\n",
       " 'wsj_0076.dp',\n",
       " 'wsj_0136.dp',\n",
       " 'wsj_0027.dp',\n",
       " 'wsj_0167.dp',\n",
       " 'wsj_0194.dp',\n",
       " 'wsj_0085.dp',\n",
       " 'wsj_0007.dp',\n",
       " 'wsj_0147.dp',\n",
       " 'wsj_0056.dp',\n",
       " 'wsj_0116.dp',\n",
       " 'wsj_0180.dp',\n",
       " 'wsj_0091.dp',\n",
       " 'wsj_0122.dp',\n",
       " 'wsj_0062.dp',\n",
       " 'wsj_0173.dp',\n",
       " 'wsj_0033.dp',\n",
       " 'wsj_0023.dp',\n",
       " 'wsj_0163.dp',\n",
       " 'wsj_0072.dp',\n",
       " 'wsj_0132.dp',\n",
       " 'wsj_0081.dp',\n",
       " 'wsj_0190.dp',\n",
       " 'wsj_0106.dp',\n",
       " 'wsj_0046.dp',\n",
       " 'wsj_0157.dp',\n",
       " 'wsj_0017.dp',\n",
       " 'wsj_0006.dp',\n",
       " 'wsj_0146.dp',\n",
       " 'wsj_0057.dp',\n",
       " 'wsj_0117.dp',\n",
       " 'wsj_0181.dp',\n",
       " 'wsj_0090.dp',\n",
       " 'wsj_0123.dp',\n",
       " 'wsj_0063.dp',\n",
       " 'wsj_0172.dp',\n",
       " 'wsj_0032.dp',\n",
       " 'wsj_0022.dp',\n",
       " 'wsj_0162.dp',\n",
       " 'wsj_0073.dp',\n",
       " 'wsj_0133.dp',\n",
       " 'wsj_0080.dp',\n",
       " 'wsj_0191.dp',\n",
       " 'wsj_0107.dp',\n",
       " 'wsj_0047.dp',\n",
       " 'wsj_0156.dp',\n",
       " 'wsj_0016.dp',\n",
       " 'wsj_0094.dp',\n",
       " 'wsj_0185.dp',\n",
       " 'wsj_0176.dp',\n",
       " 'wsj_0036.dp',\n",
       " 'wsj_0127.dp',\n",
       " 'wsj_0067.dp',\n",
       " 'wsj_0053.dp',\n",
       " 'wsj_0113.dp',\n",
       " 'wsj_0002.dp',\n",
       " 'wsj_0142.dp',\n",
       " 'wsj_0152.dp',\n",
       " 'wsj_0012.dp',\n",
       " 'wsj_0103.dp',\n",
       " 'wsj_0043.dp',\n",
       " 'wsj_0077.dp',\n",
       " 'wsj_0137.dp',\n",
       " 'wsj_0026.dp',\n",
       " 'wsj_0166.dp',\n",
       " 'wsj_0195.dp',\n",
       " 'wsj_0084.dp',\n",
       " 'wsj_0058.dp',\n",
       " 'wsj_0118.dp',\n",
       " 'wsj_0009.dp',\n",
       " 'wsj_0149.dp',\n",
       " 'wsj_0159.dp',\n",
       " 'wsj_0019.dp',\n",
       " 'wsj_0108.dp',\n",
       " 'wsj_0048.dp',\n",
       " 'wsj_0128.dp',\n",
       " 'wsj_0068.dp',\n",
       " 'wsj_0179.dp',\n",
       " 'wsj_0039.dp',\n",
       " 'wsj_0029.dp',\n",
       " 'wsj_0169.dp',\n",
       " 'wsj_0078.dp',\n",
       " 'wsj_0138.dp',\n",
       " 'wsj_0129.dp',\n",
       " 'wsj_0069.dp',\n",
       " 'wsj_0178.dp',\n",
       " 'wsj_0038.dp',\n",
       " 'wsj_0028.dp',\n",
       " 'wsj_0168.dp',\n",
       " 'wsj_0079.dp',\n",
       " 'wsj_0139.dp',\n",
       " 'wsj_0059.dp',\n",
       " 'wsj_0119.dp',\n",
       " 'wsj_0008.dp',\n",
       " 'wsj_0148.dp',\n",
       " 'wsj_0158.dp',\n",
       " 'wsj_0018.dp',\n",
       " 'wsj_0109.dp',\n",
       " 'wsj_0049.dp',\n",
       " 'wsj_0099.dp',\n",
       " 'wsj_0188.dp',\n",
       " 'wsj_0198.dp',\n",
       " 'wsj_0089.dp',\n",
       " 'wsj_0098.dp',\n",
       " 'wsj_0189.dp',\n",
       " 'wsj_0199.dp',\n",
       " 'wsj_0088.dp',\n",
       " 'wsj_0054.dp',\n",
       " 'wsj_0114.dp',\n",
       " 'wsj_0005.dp',\n",
       " 'wsj_0145.dp',\n",
       " 'wsj_0171.dp',\n",
       " 'wsj_0031.dp',\n",
       " 'wsj_0120.dp',\n",
       " 'wsj_0060.dp',\n",
       " 'wsj_0093.dp',\n",
       " 'wsj_0182.dp',\n",
       " 'wsj_0192.dp',\n",
       " 'wsj_0083.dp',\n",
       " 'wsj_0070.dp',\n",
       " 'wsj_0130.dp',\n",
       " 'wsj_0021.dp',\n",
       " 'wsj_0161.dp',\n",
       " 'wsj_0155.dp',\n",
       " 'wsj_0015.dp',\n",
       " 'wsj_0104.dp',\n",
       " 'wsj_0044.dp',\n",
       " 'wsj_0124.dp',\n",
       " 'wsj_0064.dp',\n",
       " 'wsj_0175.dp',\n",
       " 'wsj_0035.dp',\n",
       " 'wsj_0186.dp',\n",
       " 'wsj_0097.dp',\n",
       " 'wsj_0001.dp',\n",
       " 'wsj_0141.dp',\n",
       " 'wsj_0050.dp',\n",
       " 'wsj_0110.dp',\n",
       " 'wsj_0100.dp',\n",
       " 'wsj_0040.dp',\n",
       " 'wsj_0151.dp',\n",
       " 'wsj_0011.dp',\n",
       " 'wsj_0087.dp',\n",
       " 'wsj_0196.dp',\n",
       " 'wsj_0025.dp',\n",
       " 'wsj_0165.dp',\n",
       " 'wsj_0074.dp',\n",
       " 'wsj_0134.dp',\n",
       " 'wsj_0125.dp',\n",
       " 'wsj_0065.dp',\n",
       " 'wsj_0174.dp',\n",
       " 'wsj_0034.dp',\n",
       " 'wsj_0187.dp',\n",
       " 'wsj_0096.dp',\n",
       " 'wsj_0140.dp',\n",
       " 'wsj_0051.dp',\n",
       " 'wsj_0111.dp',\n",
       " 'wsj_0101.dp',\n",
       " 'wsj_0041.dp',\n",
       " 'wsj_0150.dp',\n",
       " 'wsj_0010.dp',\n",
       " 'wsj_0086.dp',\n",
       " 'wsj_0197.dp',\n",
       " 'wsj_0024.dp',\n",
       " 'wsj_0164.dp',\n",
       " 'wsj_0075.dp',\n",
       " 'wsj_0135.dp',\n",
       " 'wsj_0055.dp',\n",
       " 'wsj_0115.dp',\n",
       " 'wsj_0004.dp',\n",
       " 'wsj_0144.dp',\n",
       " 'wsj_0170.dp',\n",
       " 'wsj_0030.dp',\n",
       " 'wsj_0121.dp',\n",
       " 'wsj_0061.dp',\n",
       " 'wsj_0092.dp',\n",
       " 'wsj_0183.dp',\n",
       " 'wsj_0193.dp',\n",
       " 'wsj_0082.dp',\n",
       " 'wsj_0071.dp',\n",
       " 'wsj_0131.dp',\n",
       " 'wsj_0020.dp',\n",
       " 'wsj_0160.dp',\n",
       " 'wsj_0154.dp',\n",
       " 'wsj_0014.dp',\n",
       " 'wsj_0105.dp',\n",
       " 'wsj_0045.dp']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now get a list of the input documents\n",
    "import os\n",
    "docs = os.listdir('data/dependency_treebank')\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 50, 50)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And split train, dev, and test sets\n",
    "import random\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:int(0.5*len(docs))]\n",
    "dev_docs = docs[int(0.5*len(docs)):int(0.75*len(docs))]\n",
    "test_docs = docs[int(0.75*len(docs)):]\n",
    "len(train_docs), len(dev_docs), len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def parse_dataset(docs, dir):\n",
    "    \"\"\"\n",
    "    Parse the dependency treebank dataset.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for doc in docs:\n",
    "        np_doc = np.loadtxt(dir+doc, str, delimiter='\\t')\n",
    "        X.append(\" \".join(np_doc[:,0]))\n",
    "        y.append(\" \".join(np_doc[:,1]))\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = parse_dataset(train_docs, 'data/dependency_treebank/')\n",
    "X_test, y_test = parse_dataset(test_docs, 'data/dependency_treebank/')\n",
    "X_dev, y_dev = parse_dataset(dev_docs, 'data/dependency_treebank/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'of', 'to']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "X_vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "X_train_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "X_vectorizer.adapt(X_train_ds)\n",
    "X_vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'nn', 'in', 'nnp']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vectorizer = TextVectorization(max_tokens=100, output_sequence_length=200)\n",
    "y_train_ds = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "y_vectorizer.adapt(y_train_ds)\n",
    "y_vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "embedding_dim = 100\n",
    "with open(\"glove.6B.\"+str(embedding_dim) + \"d.txt\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "embeddings_index[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = X_vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "embedding_matrix = np.zeros((len(vocabulary)+2, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(vocabulary)+2, # Number of tokens in the vocabulary\n",
    "    embedding_dim, # Dimensions of the embedding\n",
    "    embeddings_initializer=initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training set\n",
    "X_train = X_vectorizer(np.array([[s] for s in X_train])).numpy()\n",
    "y_train = y_vectorizer(np.array([[s] for s in y_train])).numpy()\n",
    "# Transforming y_train into one-hot vectors\n",
    "one_hot_depth = np.max(y_train) + 1\n",
    "y_train = tf.one_hot(y_train, one_hot_depth).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test set\n",
    "X_test = X_vectorizer(np.array([[s] for s in X_test])).numpy()\n",
    "y_test = y_vectorizer(np.array([[s] for s in y_test])).numpy()\n",
    "# Transforming y_test into one-hot vectors\n",
    "y_test = tf.one_hot(y_test, one_hot_depth).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 100)         709700    \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirecti  (None, None, 64)         34048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, None, 35)          2275      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 746,023\n",
      "Trainable params: 36,323\n",
      "Non-trainable params: 709,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\n",
    "model.add(layers.Dense(one_hot_depth, activation=\"softmax\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.6879 - accuracy: 0.0658 - val_loss: 0.6759 - val_accuracy: 0.0161\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.6749 - accuracy: 0.0177 - val_loss: 0.6641 - val_accuracy: 0.0150\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.6622 - accuracy: 0.0173 - val_loss: 0.6526 - val_accuracy: 0.0139\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.6497 - accuracy: 0.0176 - val_loss: 0.6411 - val_accuracy: 0.0138\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.6374 - accuracy: 0.0171 - val_loss: 0.6298 - val_accuracy: 0.0135\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.6252 - accuracy: 0.0169 - val_loss: 0.6185 - val_accuracy: 0.0134\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.6131 - accuracy: 0.0163 - val_loss: 0.6072 - val_accuracy: 0.0135\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.6009 - accuracy: 0.0162 - val_loss: 0.5958 - val_accuracy: 0.0125\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.5887 - accuracy: 0.0159 - val_loss: 0.5842 - val_accuracy: 0.0119\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.5763 - accuracy: 0.0156 - val_loss: 0.5725 - val_accuracy: 0.0109\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.5638 - accuracy: 0.0148 - val_loss: 0.5607 - val_accuracy: 0.0094\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.5512 - accuracy: 0.0131 - val_loss: 0.5487 - val_accuracy: 0.0086\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.5385 - accuracy: 0.0114 - val_loss: 0.5367 - val_accuracy: 0.0078\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.5258 - accuracy: 0.0099 - val_loss: 0.5246 - val_accuracy: 0.0062\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.5129 - accuracy: 0.0083 - val_loss: 0.5124 - val_accuracy: 0.0046\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.5001 - accuracy: 0.0068 - val_loss: 0.5003 - val_accuracy: 0.0038\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.4873 - accuracy: 0.0051 - val_loss: 0.4881 - val_accuracy: 0.0031\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.4746 - accuracy: 0.0040 - val_loss: 0.4760 - val_accuracy: 0.0028\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4618 - accuracy: 0.0030 - val_loss: 0.4638 - val_accuracy: 0.0025\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.4491 - accuracy: 0.0023 - val_loss: 0.4516 - val_accuracy: 0.0021\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4364 - accuracy: 0.0020 - val_loss: 0.4393 - val_accuracy: 0.0018\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.4236 - accuracy: 0.0014 - val_loss: 0.4269 - val_accuracy: 0.0014\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.4109 - accuracy: 7.5758e-04 - val_loss: 0.4142 - val_accuracy: 0.0010\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.3980 - accuracy: 6.5657e-04 - val_loss: 0.4012 - val_accuracy: 0.0013\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.3849 - accuracy: 9.5960e-04 - val_loss: 0.3876 - val_accuracy: 0.0011\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.3713 - accuracy: 8.0808e-04 - val_loss: 0.3730 - val_accuracy: 0.0010\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.3571 - accuracy: 8.0808e-04 - val_loss: 0.3569 - val_accuracy: 9.0000e-04\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.3419 - accuracy: 8.5859e-04 - val_loss: 0.3388 - val_accuracy: 8.0000e-04\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.3251 - accuracy: 0.0011 - val_loss: 0.3182 - val_accuracy: 0.0018\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.3065 - accuracy: 0.0015 - val_loss: 0.2967 - val_accuracy: 8.0000e-04\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.2876 - accuracy: 5.5556e-04 - val_loss: 0.2789 - val_accuracy: 7.0000e-04\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.2716 - accuracy: 6.0606e-04 - val_loss: 0.2644 - val_accuracy: 6.0000e-04\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2583 - accuracy: 8.5859e-04 - val_loss: 0.2520 - val_accuracy: 0.0035\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.2466 - accuracy: 0.0033 - val_loss: 0.2410 - val_accuracy: 0.0064\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.2362 - accuracy: 0.0060 - val_loss: 0.2310 - val_accuracy: 0.0116\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.2266 - accuracy: 0.0108 - val_loss: 0.2219 - val_accuracy: 0.0143\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.2179 - accuracy: 0.0134 - val_loss: 0.2135 - val_accuracy: 0.0169\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.2099 - accuracy: 0.0160 - val_loss: 0.2059 - val_accuracy: 0.0194\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.2026 - accuracy: 0.0184 - val_loss: 0.1988 - val_accuracy: 0.0219\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.1958 - accuracy: 0.0209 - val_loss: 0.1923 - val_accuracy: 0.0244\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1895 - accuracy: 0.0232 - val_loss: 0.1863 - val_accuracy: 0.0269\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.1837 - accuracy: 0.0256 - val_loss: 0.1807 - val_accuracy: 0.0269\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.1783 - accuracy: 0.0257 - val_loss: 0.1755 - val_accuracy: 0.0294\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.1733 - accuracy: 0.0279 - val_loss: 0.1707 - val_accuracy: 0.0296\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1687 - accuracy: 0.0280 - val_loss: 0.1663 - val_accuracy: 0.0321\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.1644 - accuracy: 0.0304 - val_loss: 0.1621 - val_accuracy: 0.0322\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.1604 - accuracy: 0.0304 - val_loss: 0.1583 - val_accuracy: 0.0322\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.1568 - accuracy: 0.0305 - val_loss: 0.1548 - val_accuracy: 0.0346\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.1534 - accuracy: 0.0328 - val_loss: 0.1515 - val_accuracy: 0.0346\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.1503 - accuracy: 0.0328 - val_loss: 0.1485 - val_accuracy: 0.0347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8bbbfab50>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1024, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "28\n",
      "4\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "6\n",
      "9\n",
      "\n",
      "6\n",
      "12\n",
      "\n",
      "28\n",
      "9\n",
      "\n",
      "6\n",
      "12\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "28\n",
      "4\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "2\n",
      "8\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "28\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "21\n",
      "\n",
      "6\n",
      "10\n",
      "\n",
      "6\n",
      "15\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "13\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "9\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "16\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "28\n",
      "2\n",
      "\n",
      "28\n",
      "17\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "6\n",
      "15\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "13\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "19\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "16\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "12\n",
      "\n",
      "6\n",
      "7\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "27\n",
      "7\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "26\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "19\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "7\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "9\n",
      "\n",
      "6\n",
      "12\n",
      "\n",
      "6\n",
      "19\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "2\n",
      "2\n",
      "\n",
      "27\n",
      "6\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "7\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "9\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "19\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "7\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "13\n",
      "\n",
      "6\n",
      "12\n",
      "\n",
      "6\n",
      "9\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "16\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "10\n",
      "\n",
      "6\n",
      "17\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "6\n",
      "19\n",
      "\n",
      "6\n",
      "11\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "4\n",
      "\n",
      "28\n",
      "20\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "2\n",
      "6\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "3\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "16\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "13\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "16\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "27\n",
      "2\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n",
      "28\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, word_tag in enumerate(model.predict(X_test)[0]):\n",
    "    print(np.argmax(word_tag))\n",
    "    print(np.argmax(y_test[0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadson corp said it expects to report a thirdquarter net loss of 17 million to 19 million because of special reserves and continued low naturalgas prices the oklahoma city energy and defense concern said it will record a 75 million reserve for its defense group including a 47 million charge related to problems under a fixedprice development contract and 28 million in overhead costs that wo nt be reimbursed in addition hadson said it will write off about 35 million in costs related to international exploration leases where exploration efforts have been unsuccessful the company also cited interest costs and amortization of goodwill as factors in the loss a year earlier net income was 21 million or six cents a share on revenue of 1699 million                                                                           "
     ]
    }
   ],
   "source": [
    "vocab = X_vectorizer.get_vocabulary()\n",
    "for word in X_train[0]:\n",
    "    print(vocab[word], end=\" \")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
