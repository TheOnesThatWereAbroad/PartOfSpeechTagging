{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse.dependencygraph import DependencyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /Users/simone/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('dependency_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(will\n",
      "  (Vinken Pierre , (old (years 61)) ,)\n",
      "  (join (board the) (as (director a nonexecutive)) (Nov. 29))\n",
      "  .)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "t = dependency_treebank.parsed_sents()\n",
    "print(t[0].tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the GloVe embeddings to vectorize our data.\n",
    "\n",
    "**Uncomment to actually download!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0095.dp',\n",
       " 'wsj_0184.dp',\n",
       " 'wsj_0177.dp',\n",
       " 'wsj_0037.dp',\n",
       " 'wsj_0126.dp',\n",
       " 'wsj_0066.dp',\n",
       " 'wsj_0052.dp',\n",
       " 'wsj_0112.dp',\n",
       " 'wsj_0003.dp',\n",
       " 'wsj_0143.dp',\n",
       " 'wsj_0153.dp',\n",
       " 'wsj_0013.dp',\n",
       " 'wsj_0102.dp',\n",
       " 'wsj_0042.dp',\n",
       " 'wsj_0076.dp',\n",
       " 'wsj_0136.dp',\n",
       " 'wsj_0027.dp',\n",
       " 'wsj_0167.dp',\n",
       " 'wsj_0194.dp',\n",
       " 'wsj_0085.dp',\n",
       " 'wsj_0007.dp',\n",
       " 'wsj_0147.dp',\n",
       " 'wsj_0056.dp',\n",
       " 'wsj_0116.dp',\n",
       " 'wsj_0180.dp',\n",
       " 'wsj_0091.dp',\n",
       " 'wsj_0122.dp',\n",
       " 'wsj_0062.dp',\n",
       " 'wsj_0173.dp',\n",
       " 'wsj_0033.dp',\n",
       " 'wsj_0023.dp',\n",
       " 'wsj_0163.dp',\n",
       " 'wsj_0072.dp',\n",
       " 'wsj_0132.dp',\n",
       " 'wsj_0081.dp',\n",
       " 'wsj_0190.dp',\n",
       " 'wsj_0106.dp',\n",
       " 'wsj_0046.dp',\n",
       " 'wsj_0157.dp',\n",
       " 'wsj_0017.dp',\n",
       " 'wsj_0006.dp',\n",
       " 'wsj_0146.dp',\n",
       " 'wsj_0057.dp',\n",
       " 'wsj_0117.dp',\n",
       " 'wsj_0181.dp',\n",
       " 'wsj_0090.dp',\n",
       " 'wsj_0123.dp',\n",
       " 'wsj_0063.dp',\n",
       " 'wsj_0172.dp',\n",
       " 'wsj_0032.dp',\n",
       " 'wsj_0022.dp',\n",
       " 'wsj_0162.dp',\n",
       " 'wsj_0073.dp',\n",
       " 'wsj_0133.dp',\n",
       " 'wsj_0080.dp',\n",
       " 'wsj_0191.dp',\n",
       " 'wsj_0107.dp',\n",
       " 'wsj_0047.dp',\n",
       " 'wsj_0156.dp',\n",
       " 'wsj_0016.dp',\n",
       " 'wsj_0094.dp',\n",
       " 'wsj_0185.dp',\n",
       " 'wsj_0176.dp',\n",
       " 'wsj_0036.dp',\n",
       " 'wsj_0127.dp',\n",
       " 'wsj_0067.dp',\n",
       " 'wsj_0053.dp',\n",
       " 'wsj_0113.dp',\n",
       " 'wsj_0002.dp',\n",
       " 'wsj_0142.dp',\n",
       " 'wsj_0152.dp',\n",
       " 'wsj_0012.dp',\n",
       " 'wsj_0103.dp',\n",
       " 'wsj_0043.dp',\n",
       " 'wsj_0077.dp',\n",
       " 'wsj_0137.dp',\n",
       " 'wsj_0026.dp',\n",
       " 'wsj_0166.dp',\n",
       " 'wsj_0195.dp',\n",
       " 'wsj_0084.dp',\n",
       " 'wsj_0058.dp',\n",
       " 'wsj_0118.dp',\n",
       " 'wsj_0009.dp',\n",
       " 'wsj_0149.dp',\n",
       " 'wsj_0159.dp',\n",
       " 'wsj_0019.dp',\n",
       " 'wsj_0108.dp',\n",
       " 'wsj_0048.dp',\n",
       " 'wsj_0128.dp',\n",
       " 'wsj_0068.dp',\n",
       " 'wsj_0179.dp',\n",
       " 'wsj_0039.dp',\n",
       " 'wsj_0029.dp',\n",
       " 'wsj_0169.dp',\n",
       " 'wsj_0078.dp',\n",
       " 'wsj_0138.dp',\n",
       " 'wsj_0129.dp',\n",
       " 'wsj_0069.dp',\n",
       " 'wsj_0178.dp',\n",
       " 'wsj_0038.dp',\n",
       " 'wsj_0028.dp',\n",
       " 'wsj_0168.dp',\n",
       " 'wsj_0079.dp',\n",
       " 'wsj_0139.dp',\n",
       " 'wsj_0059.dp',\n",
       " 'wsj_0119.dp',\n",
       " 'wsj_0008.dp',\n",
       " 'wsj_0148.dp',\n",
       " 'wsj_0158.dp',\n",
       " 'wsj_0018.dp',\n",
       " 'wsj_0109.dp',\n",
       " 'wsj_0049.dp',\n",
       " 'wsj_0099.dp',\n",
       " 'wsj_0188.dp',\n",
       " 'wsj_0198.dp',\n",
       " 'wsj_0089.dp',\n",
       " 'wsj_0098.dp',\n",
       " 'wsj_0189.dp',\n",
       " 'wsj_0199.dp',\n",
       " 'wsj_0088.dp',\n",
       " 'wsj_0054.dp',\n",
       " 'wsj_0114.dp',\n",
       " 'wsj_0005.dp',\n",
       " 'wsj_0145.dp',\n",
       " 'wsj_0171.dp',\n",
       " 'wsj_0031.dp',\n",
       " 'wsj_0120.dp',\n",
       " 'wsj_0060.dp',\n",
       " 'wsj_0093.dp',\n",
       " 'wsj_0182.dp',\n",
       " 'wsj_0192.dp',\n",
       " 'wsj_0083.dp',\n",
       " 'wsj_0070.dp',\n",
       " 'wsj_0130.dp',\n",
       " 'wsj_0021.dp',\n",
       " 'wsj_0161.dp',\n",
       " 'wsj_0155.dp',\n",
       " 'wsj_0015.dp',\n",
       " 'wsj_0104.dp',\n",
       " 'wsj_0044.dp',\n",
       " 'wsj_0124.dp',\n",
       " 'wsj_0064.dp',\n",
       " 'wsj_0175.dp',\n",
       " 'wsj_0035.dp',\n",
       " 'wsj_0186.dp',\n",
       " 'wsj_0097.dp',\n",
       " 'wsj_0001.dp',\n",
       " 'wsj_0141.dp',\n",
       " 'wsj_0050.dp',\n",
       " 'wsj_0110.dp',\n",
       " 'wsj_0100.dp',\n",
       " 'wsj_0040.dp',\n",
       " 'wsj_0151.dp',\n",
       " 'wsj_0011.dp',\n",
       " 'wsj_0087.dp',\n",
       " 'wsj_0196.dp',\n",
       " 'wsj_0025.dp',\n",
       " 'wsj_0165.dp',\n",
       " 'wsj_0074.dp',\n",
       " 'wsj_0134.dp',\n",
       " 'wsj_0125.dp',\n",
       " 'wsj_0065.dp',\n",
       " 'wsj_0174.dp',\n",
       " 'wsj_0034.dp',\n",
       " 'wsj_0187.dp',\n",
       " 'wsj_0096.dp',\n",
       " 'wsj_0140.dp',\n",
       " 'wsj_0051.dp',\n",
       " 'wsj_0111.dp',\n",
       " 'wsj_0101.dp',\n",
       " 'wsj_0041.dp',\n",
       " 'wsj_0150.dp',\n",
       " 'wsj_0010.dp',\n",
       " 'wsj_0086.dp',\n",
       " 'wsj_0197.dp',\n",
       " 'wsj_0024.dp',\n",
       " 'wsj_0164.dp',\n",
       " 'wsj_0075.dp',\n",
       " 'wsj_0135.dp',\n",
       " 'wsj_0055.dp',\n",
       " 'wsj_0115.dp',\n",
       " 'wsj_0004.dp',\n",
       " 'wsj_0144.dp',\n",
       " 'wsj_0170.dp',\n",
       " 'wsj_0030.dp',\n",
       " 'wsj_0121.dp',\n",
       " 'wsj_0061.dp',\n",
       " 'wsj_0092.dp',\n",
       " 'wsj_0183.dp',\n",
       " 'wsj_0193.dp',\n",
       " 'wsj_0082.dp',\n",
       " 'wsj_0071.dp',\n",
       " 'wsj_0131.dp',\n",
       " 'wsj_0020.dp',\n",
       " 'wsj_0160.dp',\n",
       " 'wsj_0154.dp',\n",
       " 'wsj_0014.dp',\n",
       " 'wsj_0105.dp',\n",
       " 'wsj_0045.dp']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now get a list of the input documents\n",
    "import os\n",
    "docs = os.listdir('data/dependency_treebank')\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 50, 50)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And split train, dev, and test sets\n",
    "import random\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:int(0.5*len(docs))]\n",
    "dev_docs = docs[int(0.5*len(docs)):int(0.75*len(docs))]\n",
    "test_docs = docs[int(0.75*len(docs)):]\n",
    "len(train_docs), len(dev_docs), len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def parse_dataset(docs, dir):\n",
    "    \"\"\"\n",
    "    Parse the dependency treebank dataset.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for doc in docs:\n",
    "        np_doc = np.loadtxt(dir+doc, str, delimiter='\\t')\n",
    "        X.append(\" \".join(np_doc[:,0]))\n",
    "        y.append(\" \".join(np_doc[:,1]))\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = parse_dataset(train_docs, 'data/dependency_treebank/')\n",
    "X_test, y_test = parse_dataset(test_docs, 'data/dependency_treebank/')\n",
    "X_dev, y_dev = parse_dataset(dev_docs, 'data/dependency_treebank/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'of', 'to']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "sequence_length = 500\n",
    "X_vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=sequence_length)\n",
    "X_train_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "X_vectorizer.adapt(X_train_ds)\n",
    "X_vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'nn', 'in', 'nnp']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vectorizer = TextVectorization(max_tokens=100, output_sequence_length=sequence_length)\n",
    "y_train_ds = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "y_vectorizer.adapt(y_train_ds)\n",
    "y_vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "embedding_dim = 100\n",
    "with open(\"glove.6B.\"+str(embedding_dim) + \"d.txt\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "embeddings_index[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = X_vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "embedding_matrix = np.zeros((len(vocabulary)+2, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(vocabulary)+2, # Number of tokens in the vocabulary\n",
    "    embedding_dim, # Dimensions of the embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training set\n",
    "X_train = X_vectorizer(np.array([[s] for s in X_train])).numpy()\n",
    "y_train = y_vectorizer(np.array([[s] for s in y_train])).numpy()\n",
    "# Transforming y_train into one-hot vectors\n",
    "one_hot_depth = np.max(y_train) + 1\n",
    "y_train = tf.one_hot(y_train, one_hot_depth).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 500)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test set\n",
    "X_test = X_vectorizer(np.array([[s] for s in X_test])).numpy()\n",
    "y_test = y_vectorizer(np.array([[s] for s in y_test])).numpy()\n",
    "# Transforming y_test into one-hot vectors\n",
    "y_test = tf.one_hot(y_test, one_hot_depth).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  693400    \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 500, 512)         731136    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 500, 38)          19494     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 500, 38)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,444,030\n",
      "Trainable params: 1,444,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(sequence_length,)))\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Bidirectional(layers.LSTM(256, return_sequences=True)))\n",
    "model.add(layers.TimeDistributed(layers.Dense(one_hot_depth)))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.001),  metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 500, 38)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 7s 779ms/step - loss: 0.0421 - accuracy: 0.6992 - val_loss: 0.0575 - val_accuracy: 0.6612\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 7s 806ms/step - loss: 0.0415 - accuracy: 0.7071 - val_loss: 0.0567 - val_accuracy: 0.6626\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 7s 767ms/step - loss: 0.0409 - accuracy: 0.7158 - val_loss: 0.0570 - val_accuracy: 0.6701\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 8s 878ms/step - loss: 0.0403 - accuracy: 0.7183 - val_loss: 0.0584 - val_accuracy: 0.6700\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 8s 928ms/step - loss: 0.0397 - accuracy: 0.7314 - val_loss: 0.0577 - val_accuracy: 0.6734\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 8s 936ms/step - loss: 0.0391 - accuracy: 0.7324 - val_loss: 0.0573 - val_accuracy: 0.6780\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 8s 911ms/step - loss: 0.0385 - accuracy: 0.7397 - val_loss: 0.0572 - val_accuracy: 0.6796\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 9s 996ms/step - loss: 0.0379 - accuracy: 0.7437 - val_loss: 0.0568 - val_accuracy: 0.6814\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.0373 - accuracy: 0.7479 - val_loss: 0.0564 - val_accuracy: 0.6839\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0367 - accuracy: 0.7532 - val_loss: 0.0564 - val_accuracy: 0.6870\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0362 - accuracy: 0.7549 - val_loss: 0.0564 - val_accuracy: 0.6900\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.0357 - accuracy: 0.7595 - val_loss: 0.0563 - val_accuracy: 0.6912\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0352 - accuracy: 0.7612 - val_loss: 0.0558 - val_accuracy: 0.6886\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0348 - accuracy: 0.7646 - val_loss: 0.0559 - val_accuracy: 0.6929\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 9s 973ms/step - loss: 0.0343 - accuracy: 0.7671 - val_loss: 0.0540 - val_accuracy: 0.6924\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0339 - accuracy: 0.7692 - val_loss: 0.0538 - val_accuracy: 0.6924\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 9s 979ms/step - loss: 0.0334 - accuracy: 0.7731 - val_loss: 0.0536 - val_accuracy: 0.6942\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 9s 960ms/step - loss: 0.0330 - accuracy: 0.7752 - val_loss: 0.0536 - val_accuracy: 0.6924\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 9s 976ms/step - loss: 0.0326 - accuracy: 0.7772 - val_loss: 0.0534 - val_accuracy: 0.6940\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 9s 968ms/step - loss: 0.0322 - accuracy: 0.7779 - val_loss: 0.0538 - val_accuracy: 0.6958\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 8s 927ms/step - loss: 0.0319 - accuracy: 0.7806 - val_loss: 0.0536 - val_accuracy: 0.6969\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 9s 976ms/step - loss: 0.0314 - accuracy: 0.7842 - val_loss: 0.0534 - val_accuracy: 0.6948\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 9s 951ms/step - loss: 0.0311 - accuracy: 0.7845 - val_loss: 0.0534 - val_accuracy: 0.6978\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 9s 980ms/step - loss: 0.0308 - accuracy: 0.7867 - val_loss: 0.0536 - val_accuracy: 0.6978\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0305 - accuracy: 0.7891 - val_loss: 0.0539 - val_accuracy: 0.6985\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0301 - accuracy: 0.7912 - val_loss: 0.0536 - val_accuracy: 0.6997\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 9s 959ms/step - loss: 0.0299 - accuracy: 0.7936 - val_loss: 0.0536 - val_accuracy: 0.6988\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 8s 927ms/step - loss: 0.0295 - accuracy: 0.7969 - val_loss: 0.0535 - val_accuracy: 0.6985\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 8s 858ms/step - loss: 0.0292 - accuracy: 0.7980 - val_loss: 0.0535 - val_accuracy: 0.7000\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 8s 845ms/step - loss: 0.0290 - accuracy: 0.8001 - val_loss: 0.0605 - val_accuracy: 0.6918\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 7s 814ms/step - loss: 0.0288 - accuracy: 0.8000 - val_loss: 0.0604 - val_accuracy: 0.6928\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 8s 890ms/step - loss: 0.0284 - accuracy: 0.8034 - val_loss: 0.0612 - val_accuracy: 0.6947\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 8s 951ms/step - loss: 0.0282 - accuracy: 0.8051 - val_loss: 0.0592 - val_accuracy: 0.6941\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 9s 970ms/step - loss: 0.0280 - accuracy: 0.8068 - val_loss: 0.0587 - val_accuracy: 0.6944\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 8s 944ms/step - loss: 0.0276 - accuracy: 0.8100 - val_loss: 0.0590 - val_accuracy: 0.6936\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0274 - accuracy: 0.8123 - val_loss: 0.0591 - val_accuracy: 0.6934\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 10s 1s/step - loss: 0.0271 - accuracy: 0.8139 - val_loss: 0.0594 - val_accuracy: 0.6916\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0269 - accuracy: 0.8157 - val_loss: 0.0590 - val_accuracy: 0.6915\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 8s 889ms/step - loss: 0.0266 - accuracy: 0.8170 - val_loss: 0.0589 - val_accuracy: 0.6936\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 8s 902ms/step - loss: 0.0264 - accuracy: 0.8202 - val_loss: 0.0657 - val_accuracy: 0.6890\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 8s 872ms/step - loss: 0.0262 - accuracy: 0.8207 - val_loss: 0.0663 - val_accuracy: 0.6886\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 8s 890ms/step - loss: 0.0259 - accuracy: 0.8242 - val_loss: 0.0657 - val_accuracy: 0.6882\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0256 - accuracy: 0.8258 - val_loss: 0.0685 - val_accuracy: 0.6888\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 9s 993ms/step - loss: 0.0254 - accuracy: 0.8278 - val_loss: 0.0684 - val_accuracy: 0.6887\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 9s 983ms/step - loss: 0.0252 - accuracy: 0.8301 - val_loss: 0.0688 - val_accuracy: 0.6886\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 10s 1s/step - loss: 0.0249 - accuracy: 0.8310 - val_loss: 0.0694 - val_accuracy: 0.6878\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 10s 1s/step - loss: 0.0247 - accuracy: 0.8334 - val_loss: 0.0698 - val_accuracy: 0.6880\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 8s 901ms/step - loss: 0.0245 - accuracy: 0.8354 - val_loss: 0.0697 - val_accuracy: 0.6872\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 8s 855ms/step - loss: 0.0243 - accuracy: 0.8371 - val_loss: 0.0693 - val_accuracy: 0.6874\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 8s 900ms/step - loss: 0.0240 - accuracy: 0.8400 - val_loss: 0.0767 - val_accuracy: 0.6842\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 8s 893ms/step - loss: 0.0238 - accuracy: 0.8422 - val_loss: 0.0767 - val_accuracy: 0.6862\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 8s 948ms/step - loss: 0.0235 - accuracy: 0.8437 - val_loss: 0.0777 - val_accuracy: 0.6849\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 9s 963ms/step - loss: 0.0234 - accuracy: 0.8455 - val_loss: 0.0758 - val_accuracy: 0.6855\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0232 - accuracy: 0.8462 - val_loss: 0.0754 - val_accuracy: 0.6858\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 9s 993ms/step - loss: 0.0229 - accuracy: 0.8498 - val_loss: 0.0778 - val_accuracy: 0.6828\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 9s 995ms/step - loss: 0.0226 - accuracy: 0.8516 - val_loss: 0.0766 - val_accuracy: 0.6849\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 8s 912ms/step - loss: 0.0224 - accuracy: 0.8533 - val_loss: 0.0781 - val_accuracy: 0.6814\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 8s 944ms/step - loss: 0.0222 - accuracy: 0.8558 - val_loss: 0.0774 - val_accuracy: 0.6821\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 8s 955ms/step - loss: 0.0219 - accuracy: 0.8587 - val_loss: 0.1378 - val_accuracy: 0.6810\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 9s 965ms/step - loss: 0.0217 - accuracy: 0.8592 - val_loss: 0.1424 - val_accuracy: 0.6770\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 9s 962ms/step - loss: 0.0216 - accuracy: 0.8612 - val_loss: 0.1388 - val_accuracy: 0.6814\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 8s 933ms/step - loss: 0.0214 - accuracy: 0.8618 - val_loss: 0.1407 - val_accuracy: 0.6766\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 8s 871ms/step - loss: 0.0211 - accuracy: 0.8638 - val_loss: 0.1409 - val_accuracy: 0.6796\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 8s 860ms/step - loss: 0.0208 - accuracy: 0.8663 - val_loss: 0.1412 - val_accuracy: 0.6810\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 8s 923ms/step - loss: 0.0206 - accuracy: 0.8682 - val_loss: 0.1438 - val_accuracy: 0.6794\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 8s 895ms/step - loss: 0.0204 - accuracy: 0.8704 - val_loss: 0.1434 - val_accuracy: 0.6780\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 8s 856ms/step - loss: 0.0201 - accuracy: 0.8722 - val_loss: 0.1456 - val_accuracy: 0.6758\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0201 - accuracy: 0.8713 - val_loss: 0.1455 - val_accuracy: 0.6764\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 9s 931ms/step - loss: 0.0198 - accuracy: 0.8757 - val_loss: 0.1446 - val_accuracy: 0.6774\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 8s 860ms/step - loss: 0.0194 - accuracy: 0.8780 - val_loss: 0.1459 - val_accuracy: 0.6758\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 8s 854ms/step - loss: 0.0191 - accuracy: 0.8812 - val_loss: 0.1463 - val_accuracy: 0.6764\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 8s 905ms/step - loss: 0.0188 - accuracy: 0.8835 - val_loss: 0.1446 - val_accuracy: 0.6774\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 9s 1s/step - loss: 0.0186 - accuracy: 0.8849 - val_loss: 0.1438 - val_accuracy: 0.6801\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 9s 974ms/step - loss: 0.0184 - accuracy: 0.8879 - val_loss: 0.1450 - val_accuracy: 0.6764\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 8s 931ms/step - loss: 0.0181 - accuracy: 0.8904 - val_loss: 0.1468 - val_accuracy: 0.6746\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 8s 895ms/step - loss: 0.0179 - accuracy: 0.8923 - val_loss: 0.1453 - val_accuracy: 0.6761\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 8s 932ms/step - loss: 0.0176 - accuracy: 0.8934 - val_loss: 0.1407 - val_accuracy: 0.6764\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 8s 905ms/step - loss: 0.0175 - accuracy: 0.8952 - val_loss: 0.1408 - val_accuracy: 0.6759\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 8s 902ms/step - loss: 0.0171 - accuracy: 0.8978 - val_loss: 0.1414 - val_accuracy: 0.6749\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 8s 935ms/step - loss: 0.0169 - accuracy: 0.8999 - val_loss: 0.1433 - val_accuracy: 0.6718\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 8s 935ms/step - loss: 0.0169 - accuracy: 0.8989 - val_loss: 0.1424 - val_accuracy: 0.6754\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 9s 996ms/step - loss: 0.0166 - accuracy: 0.9023 - val_loss: 0.1422 - val_accuracy: 0.6764\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 8s 942ms/step - loss: 0.0163 - accuracy: 0.9046 - val_loss: 0.1439 - val_accuracy: 0.6549\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 8s 945ms/step - loss: 0.0161 - accuracy: 0.9057 - val_loss: 0.1428 - val_accuracy: 0.6577\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 8s 914ms/step - loss: 0.0159 - accuracy: 0.9077 - val_loss: 0.1433 - val_accuracy: 0.6554\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 8s 868ms/step - loss: 0.0156 - accuracy: 0.9097 - val_loss: 0.1443 - val_accuracy: 0.6710\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 8s 869ms/step - loss: 0.0154 - accuracy: 0.9120 - val_loss: 0.1396 - val_accuracy: 0.6712\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 8s 940ms/step - loss: 0.0156 - accuracy: 0.9098 - val_loss: 0.1380 - val_accuracy: 0.6614\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 9s 980ms/step - loss: 0.0152 - accuracy: 0.9140 - val_loss: 0.1399 - val_accuracy: 0.6576\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 8s 908ms/step - loss: 0.0149 - accuracy: 0.9165 - val_loss: 0.1402 - val_accuracy: 0.6584\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 8s 929ms/step - loss: 0.0147 - accuracy: 0.9178 - val_loss: 0.1413 - val_accuracy: 0.6560\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 8s 940ms/step - loss: 0.0144 - accuracy: 0.9208 - val_loss: 0.1412 - val_accuracy: 0.6563\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 7s 813ms/step - loss: 0.0141 - accuracy: 0.9226 - val_loss: 0.1413 - val_accuracy: 0.6537\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 7s 832ms/step - loss: 0.0139 - accuracy: 0.9243 - val_loss: 0.1417 - val_accuracy: 0.6539\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 8s 851ms/step - loss: 0.0137 - accuracy: 0.9254 - val_loss: 0.1422 - val_accuracy: 0.6564\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 8s 891ms/step - loss: 0.0137 - accuracy: 0.9255 - val_loss: 0.1435 - val_accuracy: 0.6518\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 8s 934ms/step - loss: 0.0134 - accuracy: 0.9275 - val_loss: 0.1452 - val_accuracy: 0.6480\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 8s 867ms/step - loss: 0.0134 - accuracy: 0.9281 - val_loss: 0.1441 - val_accuracy: 0.6513\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 8s 889ms/step - loss: 0.0131 - accuracy: 0.9306 - val_loss: 0.1447 - val_accuracy: 0.6687\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 8s 848ms/step - loss: 0.0129 - accuracy: 0.9323 - val_loss: 0.1466 - val_accuracy: 0.6506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe63facbee0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=12, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "2\n",
      "9\n",
      "\n",
      "2\n",
      "12\n",
      "\n",
      "2\n",
      "9\n",
      "\n",
      "2\n",
      "15\n",
      "\n",
      "2\n",
      "11\n",
      "\n",
      "2\n",
      "8\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "29\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "27\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "10\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "9\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "16\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "15\n",
      "\n",
      "0\n",
      "11\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "13\n",
      "\n",
      "0\n",
      "14\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "15\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "18\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "19\n",
      "\n",
      "0\n",
      "11\n",
      "\n",
      "0\n",
      "14\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "9\n",
      "\n",
      "0\n",
      "16\n",
      "\n",
      "0\n",
      "22\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "9\n",
      "\n",
      "0\n",
      "15\n",
      "\n",
      "0\n",
      "11\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "18\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "13\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "7\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "9\n",
      "\n",
      "0\n",
      "12\n",
      "\n",
      "0\n",
      "16\n",
      "\n",
      "0\n",
      "15\n",
      "\n",
      "0\n",
      "11\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, word_tag in enumerate(model.predict(X_test)[0]):\n",
    "    print(np.argmax(word_tag))\n",
    "    print(np.argmax(y_test[0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "companies listed below reported quarterly profit substantially different from the average of analysts estimates the companies are followed by at least three analysts and had a minimum fivecent change in actual earnings per share estimated and actual results involving losses are omitted the percent difference compares actual profit with the 30day estimate where at least three analysts have issues forecasts in the past 30 days otherwise actual profit is compared with the 300day estimate                                                                                                                               "
     ]
    }
   ],
   "source": [
    "vocab = X_vectorizer.get_vocabulary()\n",
    "for word in X_train[0]:\n",
    "    print(vocab[word], end=\" \")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
